{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsun75/dataAnalysis/blob/main/3_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%ED%9A%8C%EA%B7%80_2_%EC%95%84%ED%8C%8C%ED%8A%B8%EC%8B%A4%EA%B1%B0%EB%9E%98%EA%B0%80%EC%98%88%EC%B8%A1_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrWaMW2VUFZ"
      },
      "source": [
        "# [실습] 아파트 실거래 가격 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUh1Um9LZ1Wj"
      },
      "source": [
        "- 서울/부산 지역의 아파트 관련 정보를 바탕으로 실거래가 예측\n",
        "- 데이콘 대회 문제: 직방 거래 데이터(2016~2017)\n",
        "- 참고: https://dacon.io/competitions/official/21265/overview/description\n",
        "\n",
        "* 사용 데이터\n",
        "    - 아파트 실거래가 데이터(아파트실거래가격.csv\n",
        "    - 행정구역분류 데이터(한국행정구역분류.xlsx)\n",
        "    - 공원 데이터(park.csv)\n",
        "    - 어린이집 데이터(day_care_center.csv)\n",
        "    - 테스트 데이터(아파트실거래가격_test.csv)\n",
        "* 사용 모델\n",
        "    - 앙상블 모델(RandomForestRegressor, xgboost, lightgbm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zaM51mVVZsg"
      },
      "source": [
        "### #그래프에서 한글사용하는 방법\n",
        "- **(코랩에서)한글폰트 설치하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4FxW-gYUTHh",
        "outputId": "e5b9d1db-7139-4270-f0d1-a806cb050f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feature-engine\n",
            "  Downloading feature_engine-1.7.0-py2.py3-none-any.whl (344 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/344.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/344.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.3/344.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from feature-engine) (1.25.2)\n",
            "Collecting pandas>=2.2.0 (from feature-engine)\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.4.0 (from feature-engine)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from feature-engine) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from feature-engine) (0.14.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature-engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature-engine) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->feature-engine) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.0->feature-engine) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.0->feature-engine) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature-engine) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.11.1->feature-engine) (24.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.11.1->feature-engine) (1.16.0)\n",
            "Installing collected packages: scikit-learn, pandas, feature-engine\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed feature-engine-1.7.0 pandas-2.2.2 scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U feature-engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p77GneRQVaKD",
        "outputId": "c1d788de-d4ff-442c-c843-66ffaa2e56bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 10.3 MB of archives.\n",
            "After this operation, 34.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-nanum all 20200506-1 [10.3 MB]\n",
            "Fetched 10.3 MB in 0s (38.9 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20200506-1_all.deb ...\n",
            "Unpacking fonts-nanum (20200506-1) ...\n",
            "Setting up fonts-nanum (20200506-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "# 코랩에서 위 코드를 실행시킨 후  반드시 코랩 메뉴: \"런타임>세션 다시 시작\" 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIUUlV7_VepQ"
      },
      "outputs": [],
      "source": [
        "# 코랩에서 한글 폰트 종류와 이름이 win과 다를 수 있다!!!\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams.update({'font.family': 'NanumGothic',\n",
        "                     'font.size': 12,\n",
        "                     'figure.figsize': (10, 5),\n",
        "                     'axes.unicode_minus':  False }) # 폰트 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQcBa3enVgaW"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU7oDHnHVhwE"
      },
      "source": [
        "## 1.데이터 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "Kqy_4pToUTHV",
        "outputId": "ed0ba6d9-c796-4dc0-eb54-63f8544b33a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '아파트실거래가격.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-03989e53f113>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 데이터 가져오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"아파트실거래가격.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '아파트실거래가격.csv'"
          ]
        }
      ],
      "source": [
        "# 데이터 가져오기\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"아파트실거래가격.csv\", engine = \"python\", encoding = \"utf8\")\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THl-0fv6WIXn"
      },
      "source": [
        "## 2.데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB8JhV5EWJVn"
      },
      "source": [
        "### 2-1. 기본 정보 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grb0dvkbWMQD"
      },
      "source": [
        "- 데이터 요약정보"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW5ZSwXsWQAg"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJa56Gy2WQJM"
      },
      "source": [
        "- 데이터의 지역 확인(city: 서울특별시', '부산광역시)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rk3Jy7YWfzv"
      },
      "outputs": [],
      "source": [
        "df.city.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d2n4AOAW1CU"
      },
      "outputs": [],
      "source": [
        "df.city.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhT_sBlBXJQr"
      },
      "source": [
        "- 거래 년도 확인(transaction_year_month: 201601~201711)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtaF7_XIUTHV"
      },
      "outputs": [],
      "source": [
        "df.transaction_year_month.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA3K4BPlXNT9"
      },
      "outputs": [],
      "source": [
        "# 거래발생년월별 데이터 건수\n",
        "data = df.transaction_year_month.value_counts()\n",
        "data = data.sort_index()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdt3i8ZGZJ2X"
      },
      "source": [
        "- 특정 아파트 거래내역 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb6Kdu1OUTHV"
      },
      "outputs": [],
      "source": [
        "df.query(\"apartment_id == 1878\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drmvtyPGcO7i"
      },
      "source": [
        "### 2-3.데이터 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5iOhdcMcVdU"
      },
      "source": [
        "#### 1) 필요 데이터 처리(파생변수 추가, 변수 삭제, 값 변경)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq6ueoNRUTHW"
      },
      "source": [
        "#### #행정구역(시군구) 변수 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-YcmjucUTHW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# 참조 데이터 불러오기\n",
        "ref_df = pd.read_excel(\"한국행정구역분류.xlsx\",\n",
        "                       sheet_name = \"법정동코드 연계 자료분석용\",\n",
        "                       header = 1)\n",
        "\n",
        "ref_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7-o3fTWUTHW"
      },
      "outputs": [],
      "source": [
        "# ref_df 필터링\n",
        "ref_df = ref_df.loc[ref_df['시도'].isin(['서울특별시', '부산광역시'])] # 서울특별시와 부산광역시 레코드만 가져옴\n",
        "ref_df = ref_df[['시도', '시군구', '법정동']]\n",
        "ref_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VX654vdUTHW"
      },
      "outputs": [],
      "source": [
        "# ref_df에 포함되어 있는 시도-읍면동과 df에 포함되어 있는 시도-읍면동이 일치하지 않는 경우를 확인해야 함\n",
        "ref_df_loc = ref_df['시도'] + '-' + ref_df['법정동']\n",
        "df_loc = (df['city'] + '-' + df['dong']).unique()\n",
        "\n",
        "import numpy as np\n",
        "np.isin(df_loc, ref_df_loc) # df_loc가 ref_df_loc에 포함되지 않는 경우가 있음을 확인\n",
        "\n",
        "# 포함되지 않는 동 목록 확인: 전부 부산광역시이며 읍 단위인 경우에 이러한 문제가 발생함을 확인\n",
        "df_loc[~np.isin(df_loc, ref_df_loc)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDee1tHNUTHW"
      },
      "outputs": [],
      "source": [
        "# 장안읍 예시:\n",
        "ref_df.loc[ref_df['법정동'].str.contains('장안')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9HlmVeyUTHX"
      },
      "outputs": [],
      "source": [
        "# 시도와 법정동이 완전히 똑같은 행이 있어, 이를 제거함\n",
        "ref_df = ref_df.drop_duplicates(subset = ['시도', '법정동'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpUSPypfUTHX"
      },
      "outputs": [],
      "source": [
        "# 결론: df의 dong에 리가 붙어있으면 제거해야 함\n",
        "df['dong'] = df['dong'].str.split(' ', expand = True).iloc[:, 0]\n",
        "\n",
        "# 재확인: 정상적으로 작동함을 확인\n",
        "df_loc = (df['city'] + '-' + df['dong']).unique()\n",
        "df_loc[~np.isin(df_loc, ref_df_loc)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9m9Y3i2UTHX"
      },
      "outputs": [],
      "source": [
        "# df와 ref_df 병합\n",
        "df = pd.merge(df, ref_df, left_on = ['city', 'dong'], right_on = ['시도', '법정동'])\n",
        "\n",
        "# ref_df에 있던 불필요한 변수 제거\n",
        "df.drop(['시도', '법정동'], axis = 1, inplace = True)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvze7DtmfG63"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKn0AcuIUTHX"
      },
      "source": [
        "#### #불필요한 변수 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNwiCv4xUTHX"
      },
      "outputs": [],
      "source": [
        "df.drop(['transaction_id', 'addr_kr'], axis = 1, inplace = True) # transaction_id는 인덱스와 동일하므로 삭제해도 무방"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq1keHqoUTHX"
      },
      "outputs": [],
      "source": [
        "# apartment_id는 id지만 어느정도 사용이 가능할 것이라 보임 (완전히 유니크하지 않으므로)\n",
        "df['apartment_id'].value_counts().sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F1cws-4b_nR"
      },
      "outputs": [],
      "source": [
        "# 거래가 데이터가 많은 아파트\n",
        "df.query(\"apartment_id == 10939\").head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LokM3-i3UTHX"
      },
      "source": [
        "#### #파생변수 생성\n",
        "- age : 건물 나이(2018년도 기준)\n",
        "- transaction_year : 거래 년\n",
        "- transaction_month: 거래 월\n",
        "- Seoul : 서울 여부\n",
        "- floor_level : 층(floor)별 가격 구간"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snxRGkNXUTHX"
      },
      "outputs": [],
      "source": [
        "# 건축된지 얼마나 되었는지를 나타내는 변수로 변환\n",
        "# 최근거래일자가 201711이므로 2018년을 기준으로 한다.\n",
        "df['age'] = 2018 - df['year_of_completion']\n",
        "df.drop('year_of_completion', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwzmNI9PUTHX"
      },
      "outputs": [],
      "source": [
        "# 거래 년도 추출\n",
        "# str accessor를 사용하기 위해, 타입 변경\n",
        "df['transaction_year_month'] = df['transaction_year_month'].astype(str)\n",
        "\n",
        "df['transaction_year'] = df['transaction_year_month'].str[:4].astype(int)\n",
        "df['transaction_month'] = df['transaction_year_month'].str[4:].astype(int)\n",
        "df.drop('transaction_year_month', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKzsEM4NUTHX"
      },
      "outputs": [],
      "source": [
        "# 도시가 둘 뿐이므로 서울특별시인지 나타내는 이진 변수 생성\n",
        "df['Seoul'] = (df['city'] == \"서울특별시\").astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWB7Wr9WUTHY"
      },
      "outputs": [],
      "source": [
        "# floor 구간화 필요: 2층과 3층의 가격 차이가 있을까?\n",
        "# 대략적인 구간 확인: 1층 이하, 1 ~ 25층, 26층 ~ 49층, 50층 이상\n",
        "df.groupby(['floor'])['transaction_real_price'].mean().plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jabEDYZLUTHY"
      },
      "outputs": [],
      "source": [
        "# 층 수준을 생성한 뒤, 차이를 봄\n",
        "def floor_level_converter(x):\n",
        "    if x <= 15:\n",
        "        return \"low\"\n",
        "    elif x <= 25:\n",
        "        return \"middle\"\n",
        "    elif x <= 51:\n",
        "        return \"high\"\n",
        "    else:\n",
        "        return \"very_high\"\n",
        "\n",
        "df['floor_level'] = df['floor'].apply(floor_level_converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z71OBym4UTHY"
      },
      "outputs": [],
      "source": [
        "# 저층 그룹에 아파트의 층에 따른 가격 분포 확인 => 층에 따른 차이가 크지 않음\n",
        "df.loc[df['floor_level'] == 'low'].boxplot(column = 'transaction_real_price', by = 'floor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBy2-TLwUTHY"
      },
      "outputs": [],
      "source": [
        "# 중층 그룹에 아파트의 층에 따른 가격 분포 확인 => 층에 따른 차이가 크지 않음\n",
        "df.loc[df['floor_level'] == 'middle'].boxplot(column = 'transaction_real_price', by = 'floor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAqUUOayUTHY"
      },
      "outputs": [],
      "source": [
        "# 고층 그룹에 아파트의 층에 따른 가격 분포 확인 => 층에 따른 차이가 크지 않음\n",
        "df.loc[df['floor_level'] == 'high'].boxplot(column = 'transaction_real_price', by = 'floor')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERd9AiQfUTHY"
      },
      "outputs": [],
      "source": [
        "# 초고층 그룹에 아파트의 층에 따른 가격 분포 확인 => 층이 높으면 높을수록 가격이 더 오름 (단, 72층을 기준으로 그런 현상이 두드러짐)\n",
        "df.loc[df['floor_level'] == 'very_high'].boxplot(column = 'transaction_real_price', by = 'floor')\n",
        "\n",
        "# 결론: floor_level 변수는 유의하나, floor 변수는 초고층에서만 유의\n",
        "# 따라서 very high를 세분화하거나, floor_level 변수와 floor 변수를 같이 사용하고 교호작용을 고려할 수 있는 트리 계열의 모델 사용이 적합\n",
        "# 여기서는 very high를 세분화하기로 결정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uveu9uaeUTHY"
      },
      "outputs": [],
      "source": [
        "# 층 수준을 생성한 뒤, 차이를 봄\n",
        "def floor_level_converter(x):\n",
        "    if x <= 15:\n",
        "        return \"low\"\n",
        "    elif x <= 25:\n",
        "        return \"middle\"\n",
        "    elif x <= 51:\n",
        "        return \"high\"\n",
        "    elif x <= 71: # 새로운 조건 추가\n",
        "        return \"very_high\"\n",
        "    else:\n",
        "        return \"very_very_high\"\n",
        "\n",
        "df['floor_level'] = df['floor'].apply(floor_level_converter)\n",
        "df.drop('floor', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AywpR6-UTHY"
      },
      "source": [
        "#### 시세 변수 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiIZHz4pUTHY"
      },
      "outputs": [],
      "source": [
        "# 구별 전체 평균 시세 부착\n",
        "mean_price_per_gu = df.groupby(['city', '시군구'], as_index = False)['transaction_real_price'].mean()\n",
        "mean_price_per_gu.rename({'transaction_real_price':'구별_전체_평균_시세'}, axis = 1, inplace = True)\n",
        "df = pd.merge(df, mean_price_per_gu, on = ['city', '시군구'])\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKkWEK7tUTHY"
      },
      "outputs": [],
      "source": [
        "# 구별 작년 시세 부착\n",
        "# price_per_gu_and_year 변수에 직접 수정을 하므로, df가 변경되는 것을 방지하기 위해, df.copy().groupby~를 사용\n",
        "price_per_gu_and_year = df.copy().groupby(['city', '시군구', 'transaction_year'], as_index = False)['transaction_real_price'].agg(['mean', 'count'])\n",
        "price_per_gu_and_year = price_per_gu_and_year.reset_index().rename({\"mean\":\"구별_작년_평균_시세\", \"count\":\"구별_작년_거래량\"}, axis = 1)\n",
        "\n",
        "price_per_gu_and_year['transaction_year'] += 1 # 작년것을 붙이기 위해, 1을 더함\n",
        "df = pd.merge(df, price_per_gu_and_year, on = ['city', '시군구', 'transaction_year'], how = 'left') # 작년 기록이 없어서 붙지 않는 것을 방지하기 위해, how = 'left'로 설정\n",
        "df['구별_작년_거래량'].fillna(0, inplace = True) # 구별 작년 거래 데이터가 없다는 것은, 구별 작년 거래량이 0이라는 이야기이므로 fillna(0)을 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_AgD_Z4UTHZ"
      },
      "outputs": [],
      "source": [
        "# 아파트별 평균 시세 부착\n",
        "price_per_aid = df.copy().groupby(['apartment_id'], as_index = False)['transaction_real_price'].mean()\n",
        "price_per_aid.rename({\"transaction_real_price\":\"아파트별_평균가격\"}, axis = 1, inplace = True)\n",
        "\n",
        "df = pd.merge(df, price_per_aid, on = ['apartment_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eAw2TVFgQcP"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUeN_Eb3UTHZ"
      },
      "source": [
        "#### 2) 외부 데이터 부착"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTLsrUC0UTHZ"
      },
      "source": [
        "#### #공원 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCS9zWqlUTHZ"
      },
      "outputs": [],
      "source": [
        "park_df = pd.read_csv(\"park.csv\")\n",
        "park_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w3N-R7gUTHZ"
      },
      "outputs": [],
      "source": [
        "# 모든 값을 고려하는 것은 불가능하고, 그리 좋은 접근이 아닌 것으로 보임\n",
        "park_df['park_exercise_facility'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsuWz2QBUTHZ"
      },
      "outputs": [],
      "source": [
        "park_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmsBrw2EUTHZ"
      },
      "outputs": [],
      "source": [
        "# 따라서 결측을 0으로, 결측이 아니면 1로 변환\n",
        "facility_cols = ['park_exercise_facility', 'park_entertainment_facility', 'park_benefit_facility', 'park_cultural_facitiy', 'park_facility_other']\n",
        "for col in facility_cols:\n",
        "    park_df.loc[park_df[col].notnull(), col] = 1\n",
        "    park_df.loc[park_df[col].isnull(), col] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9isVjJFUTHf"
      },
      "outputs": [],
      "source": [
        "# 동별 공원 수\n",
        "num_park_per_dong = park_df.groupby(['city', 'gu', 'dong'], as_index = False)['park_name'].count()\n",
        "num_park_per_dong.rename({\"park_name\":\"공원수\"}, axis = 1, inplace = True)\n",
        "\n",
        "# 동별 공원에 배치된 facilty 수\n",
        "num_facilty_per_dong = park_df.groupby(['city', 'gu', 'dong'], as_index = False)[facility_cols].sum()\n",
        "num_facilty_per_dong.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E57pm-mpUTHg"
      },
      "outputs": [],
      "source": [
        "# 공원 데이터 부착\n",
        "df = pd.merge(df, num_park_per_dong, left_on = ['city', '시군구', 'dong'], right_on = ['city', 'gu', 'dong'], how = 'left')\n",
        "df['공원수'].fillna(0, inplace = True)\n",
        "df.drop('gu', axis = 1, inplace = True)\n",
        "\n",
        "df = pd.merge(df, num_facilty_per_dong, left_on = ['city', '시군구', 'dong'], right_on = ['city', 'gu', 'dong'], how = 'left')\n",
        "df[facility_cols].fillna(0, inplace = True)\n",
        "df.drop('gu', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_YxGV9UTHg"
      },
      "source": [
        "#### #어린이집 데이터 부착"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HfrKbObUTHg"
      },
      "outputs": [],
      "source": [
        "day_care_center_df = pd.read_csv('day_care_center.csv')\n",
        "day_care_center_df.head()\n",
        "\n",
        "# 가설: 같은 어린이집이어도 종류가 다르면 아파트 가격에 다르게 영향을 줄 것이다. 가령, 가정 어린이집보단 국공립어린이집이 더 인기가 좋을 것이다.\n",
        "# 또한, 아이가 있는 부모라면, 어린이집 수와 케어 가능한 아이의 수 등만 보고 아파트 구매를 결정하지, 각 어린이집에 CCTV 개수가 몇개인지 등까진 파악하지 않을 것이다.\n",
        "# 따라서 구 및 유형별 어린이집 수와 케어 가능한 아이 수만 집계"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbbgEG-WUTHg"
      },
      "outputs": [],
      "source": [
        "# 불필요한 변수 제거\n",
        "day_care_center_df = day_care_center_df[['city', 'gu', 'day_care_type', 'day_care_baby_num']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKoR_EefUTHg"
      },
      "outputs": [],
      "source": [
        "dummy_day_care_type = pd.get_dummies(day_care_center_df['day_care_type'], drop_first = False)\n",
        "dummy_day_care_type = dummy_day_care_type.add_prefix(\"어린이집유형_\")\n",
        "\n",
        "day_care_center_df = pd.concat([day_care_center_df, dummy_day_care_type], axis = 1)\n",
        "day_care_center_df.drop('day_care_type', axis = 1, inplace = True)\n",
        "day_care_center_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaFduyjhUTHg"
      },
      "outputs": [],
      "source": [
        "aggregated_day_care_center_df = day_care_center_df.groupby(['city', 'gu'], as_index = False)[day_care_center_df.columns[2:]].sum()\n",
        "aggregated_day_care_center_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAGE3iTxUTHg"
      },
      "outputs": [],
      "source": [
        "# 어린이집 데이터 부착\n",
        "df = pd.merge(df, aggregated_day_care_center_df, left_on = ['city', '시군구'], right_on = ['city', 'gu'], how = 'left')\n",
        "df[aggregated_day_care_center_df.columns].fillna(0, inplace = True)\n",
        "df.drop('gu', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBKyyDF_jEEN"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBSNRcL0he9M"
      },
      "source": [
        "## 3.모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzSuvoHpUTHg"
      },
      "source": [
        "### 3-1. 학습용 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1w3wF0gUTHg"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['apartment_id', 'city', 'dong', 'jibun', 'apt', 'transaction_date', 'transaction_real_price', '시군구', 'transaction_year', 'transaction_month'], axis = 1)\n",
        "Y = df['transaction_real_price']\n",
        "X.head()\n",
        "X.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mfwUa3ghj28"
      },
      "source": [
        "### 3-2. 학습용, 테스트 데이터 분리하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPWihmmuUTHh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Train_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y)\n",
        "Train_X.shape # 샘플 대비 특징이 매우 적음 => 큰 부담없이 더미화 가능\n",
        "# 컬럼 타입이 섞여 있으므로, 트리 계열의 모델이 적절한 것으로 판단됨\n",
        "# 또한, 샘플이 충분히 많으므로 트리 뿐만 아니라 트리 기반의 앙상블도 적절할 것으로 보임\n",
        "# Tip. Decision Tree 계열의 모델은 상대적으로 전처리나 탐색의 부담이 제일 적은 모델임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1C5HnC-htQV"
      },
      "source": [
        "#### #인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7JWihhWUTHh"
      },
      "outputs": [],
      "source": [
        "# 더미화\n",
        "# from feature_engine.categorical_encoders import OneHotCategoricalEncoder as OHE\n",
        "from feature_engine.encoding import OneHotEncoder as OHE\n",
        "\n",
        "dummy_model = OHE(variables = ['floor_level'],\n",
        "                 drop_last = False)\n",
        "\n",
        "dummy_model.fit(Train_X)\n",
        "\n",
        "Train_X = dummy_model.transform(Train_X)\n",
        "Test_X = dummy_model.transform(Test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dqHIL6sUTHh"
      },
      "outputs": [],
      "source": [
        "# 변수 부착 과정에서 생성된 결측 대체\n",
        "from sklearn.impute import SimpleImputer as SI\n",
        "imputer = SI().fit(Train_X)\n",
        "Train_X = pd.DataFrame(imputer.transform(Train_X), columns = Train_X.columns)\n",
        "Test_X = pd.DataFrame(imputer.transform(Test_X), columns = Test_X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PCV3b8ZjZqN"
      },
      "source": [
        "### 3-3.모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BY_uypMkguC"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T817jlubUTHh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.feature_selection import *\n",
        "from sklearn.ensemble import RandomForestRegressor as RFR\n",
        "from xgboost import XGBRegressor as XGB\n",
        "from lightgbm import LGBMRegressor as LGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oep1wju7UTHh"
      },
      "outputs": [],
      "source": [
        "model_parameter_dict = dict()\n",
        "RFR_parameter_grid = ParameterGrid({\"max_depth\":[3, 4, 5],\n",
        "                                   \"n_estimators\": [100, 200]})\n",
        "\n",
        "XL_parameter_grid = ParameterGrid({\"max_depth\":[3, 4, 5],\n",
        "                                  \"n_estimators\": [100, 200],\n",
        "                                  \"learning_rate\": [0.05, 0.1, 0.2]})\n",
        "\n",
        "model_parameter_dict[RFR] = RFR_parameter_grid\n",
        "model_parameter_dict[XGB] = XL_parameter_grid\n",
        "model_parameter_dict[LGB] = XL_parameter_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS8AJ23YUTHh"
      },
      "outputs": [],
      "source": [
        "# 출력을 위한 max_iter_num 계산\n",
        "max_iter_num = len(range(20, 4, -5)) * len(model_parameter_dict) * len(XL_parameter_grid) ** 2\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as MAE\n",
        "best_score = 9999999999\n",
        "iteration_num = 0\n",
        "for k in range(20, 4, -5):\n",
        "    selector = SelectKBest(mutual_info_regression, k = k).fit(Train_X, Train_Y)\n",
        "    s_Train_X = selector.transform(Train_X)\n",
        "    s_Test_X = selector.transform(Test_X)\n",
        "\n",
        "\n",
        "    for model_func in model_parameter_dict.keys():\n",
        "        for parameter in model_parameter_dict[model_func]:\n",
        "            model = model_func(**parameter).fit(s_Train_X, Train_Y) # Light GBM은 DataFrame의 컬럼 이름에 대한 제약이 있어, ndarray를 입력으로 사용\n",
        "            pred_Y = model.predict(s_Test_X)\n",
        "            score = MAE(Test_Y, pred_Y)\n",
        "\n",
        "            if score < best_score:\n",
        "                print(k, model_func, parameter, score)\n",
        "                best_score = score\n",
        "                best_model_func = model_func\n",
        "                best_parameter = parameter\n",
        "                best_selector = selector\n",
        "\n",
        "            iteration_num += 1\n",
        "            print(\"iter_num:{}/{}, score: {}, best_score: {}\".format(iteration_num, max_iter_num, round(score, 2), round(best_score, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh4ERwpVUTHi"
      },
      "outputs": [],
      "source": [
        "final_X = pd.concat([Train_X, Test_X], axis = 0, ignore_index = True)\n",
        "final_Y = pd.concat([Train_Y, Test_Y], axis = 0, ignore_index = True)\n",
        "\n",
        "final_model = best_model_func(**best_parameter).fit(best_selector.transform(final_X), final_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLVw_z-pUTHi"
      },
      "source": [
        "### 파이프라인 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TUh34Z2UTHi"
      },
      "outputs": [],
      "source": [
        "def pipeline(new_data, ref_df, model, selector, mean_price_per_gu, num_park_per_dong, num_facilty_per_dong, aggregated_day_care_center_df, imputer, dummy_model):\n",
        "    ## 변수 변환 및 부착\n",
        "    new_data['dong'] = new_data['dong'].str.split(' ', expand = True).iloc[:, 0] # dong에 리가 붙어있으면 제거\n",
        "\n",
        "    new_data = pd.merge(new_data, ref_df, left_on = ['city', 'dong'], right_on = ['시도', '법정동']) # 시군구 부착\n",
        "\n",
        "    new_data.drop(['시도', '법정동', 'transaction_id', 'addr_kr'], axis = 1, inplace = True) # 불필요한 변수 제거\n",
        "\n",
        "    # age 변수 부착\n",
        "    new_data['age'] = 2018 - new_data['year_of_completion']\n",
        "    new_data.drop('year_of_completion', axis = 1, inplace = True)\n",
        "\n",
        "    # 거래 년월 부착\n",
        "    new_data['transaction_year_month'] = new_data['transaction_year_month'].astype(str)\n",
        "    new_data['transaction_year'] = new_data['transaction_year_month'].str[:4].astype(int)\n",
        "    new_data['transaction_month'] = new_data['transaction_year_month'].str[4:].astype(int)\n",
        "    new_data.drop('transaction_year_month', axis = 1, inplace = True)\n",
        "\n",
        "    # Seoul 생성\n",
        "    new_data['Seoul'] = (new_data['city'] == \"서울특별시\").astype(int)\n",
        "\n",
        "    # floor_level 변수 생성\n",
        "    new_data['floor_level'] = new_data['floor'].apply(floor_level_converter)\n",
        "    new_data.drop('floor', axis = 1, inplace = True)\n",
        "\n",
        "    # 시세 관련 변수 추가\n",
        "    new_data = pd.merge(new_data, mean_price_per_gu, on = ['city', '시군구'])\n",
        "    new_data = pd.merge(new_data, price_per_gu_and_year, on = ['city', '시군구', 'transaction_year'], how = 'left')\n",
        "    new_data['구별_작년_거래량'].fillna(0, inplace = True) # 구별 작년 거래 데이터가 없다는 것은, 구별 작년 거래량이 0이라는 이야기이므로 fillna(0)을 수행\n",
        "\n",
        "    new_data = pd.merge(new_data, price_per_aid, on = ['apartment_id'], how = 'left')\n",
        "\n",
        "\n",
        "    # 공원 데이터 부착\n",
        "    new_data = pd.merge(new_data, num_park_per_dong, left_on = ['city', '시군구', 'dong'], right_on = ['city', 'gu', 'dong'], how = 'left')\n",
        "    new_data['공원수'].fillna(0, inplace = True)\n",
        "    new_data.drop('gu', axis = 1, inplace = True)\n",
        "\n",
        "    new_data = pd.merge(new_data, num_facilty_per_dong, left_on = ['city', '시군구', 'dong'], right_on = ['city', 'gu', 'dong'], how = 'left')\n",
        "\n",
        "    facility_cols = ['park_exercise_facility', 'park_entertainment_facility', 'park_benefit_facility', 'park_cultural_facitiy', 'park_facility_other']\n",
        "    new_data[facility_cols].fillna(0, inplace = True)\n",
        "    new_data.drop('gu', axis = 1, inplace = True)\n",
        "\n",
        "    # 어린이집 데이터 부착\n",
        "    new_data = pd.merge(new_data, aggregated_day_care_center_df, left_on = ['city', '시군구'], right_on = ['city', 'gu'], how = 'left')\n",
        "    new_data[aggregated_day_care_center_df.columns].fillna(0, inplace = True)\n",
        "    new_data.drop('gu', axis = 1, inplace = True)\n",
        "\n",
        "    # 특징 추출 ('transaction_real_price'는 drop 대상에서 제외)\n",
        "    X = new_data.drop(['apartment_id', 'city', 'dong', 'jibun', 'apt', 'transaction_date', '시군구', 'transaction_year', 'transaction_month'], axis = 1)\n",
        "\n",
        "    # 더미화\n",
        "    X = dummy_model.transform(X)\n",
        "\n",
        "    # 결측 대체\n",
        "    X = imputer.transform(X)\n",
        "\n",
        "    # 특징 선택\n",
        "    X = selector.transform(X)\n",
        "\n",
        "    return model.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjxfeqkpUTHi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pipeline_element = {\"ref_df\": ref_df,\n",
        "                   \"model\":final_model,\n",
        "                   \"selector\":best_selector,\n",
        "                   \"mean_price_per_gu\":mean_price_per_gu,\n",
        "                   \"num_park_per_dong\":num_park_per_dong,\n",
        "                   \"num_facilty_per_dong\":num_facilty_per_dong,\n",
        "                   \"aggregated_day_care_center_df\":aggregated_day_care_center_df,\n",
        "                   \"imputer\":imputer,\n",
        "                   \"dummy_model\":dummy_model,\n",
        "                   \"pipeline\":pipeline}\n",
        "\n",
        "with open(\"아파트실거래가예측모델.pckl\", \"wb\") as f:\n",
        "    pickle.dump(pipeline_element, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrMpFrzrUTHi"
      },
      "outputs": [],
      "source": [
        "with open(\"아파트실거래가예측모델.pckl\", \"rb\") as f:\n",
        "    pipeline_element = pickle.load(f)\n",
        "\n",
        "ref_df = pipeline_element[\"ref_df\"]\n",
        "model = pipeline_element[\"model\"]\n",
        "selector = pipeline_element[\"selector\"]\n",
        "mean_price_per_gu = pipeline_element[\"mean_price_per_gu\"]\n",
        "num_park_per_dong = pipeline_element[\"num_park_per_dong\"]\n",
        "num_facilty_per_dong = pipeline_element[\"num_facilty_per_dong\"]\n",
        "aggregated_day_care_center_df = pipeline_element[\"aggregated_day_care_center_df\"]\n",
        "imputer = pipeline_element[\"imputer\"]\n",
        "dummy_model = pipeline_element[\"dummy_model\"]\n",
        "pipeline = pipeline_element[\"pipeline\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LTzsfqOlcWu"
      },
      "source": [
        "### 3-4.모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeLsINydUTHi"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"아파트실거래가격_test.csv\")\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vba0niFLUTHj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "output = pipeline(test_df, ref_df, model, selector, mean_price_per_gu, num_park_per_dong, num_facilty_per_dong, aggregated_day_care_center_df, imputer, dummy_model)\n",
        "result = pd.Series(output, index = test_df['transaction_id'])\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}